---
title: "Hedonic Regression"
author: "Bianca Kolakovic"
date: "28/09/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





```{r read}
parra<- read.csv('Parramatta - Sheet1.csv')
attach(parra)
dim(parra)

```
There are 212 observations with 9 variables in the dataset.

```{r read2}
names(parra)
```
From the variables shown, we do not require the address column or the location column for analysis so we can simply remove them.
```{r read3}
parra_new<- parra[,-1]
names(parra_new)
```

```{r read4}
parra_new1<- parra_new[,-1]
names(parra_new1)
```
### Graphical Representation
```{r read5}
pairs(parra_new1, panel=panel.smooth)
```
```{r read6}
cor(parra_new1)
```


## Multiple Linear Regression

### Using all variables
```{r linear1}
multipleLinearRegression1<- lm(Property.Price.... ~ Bedrooms + Bathrooms + 
                                 Car.space + Lot.size..m2. + Distance..km.)
summary(multipleLinearRegression1)
```
All variables appear to be significant except the carspace variable, so we will remove this in the next model to see if it will improve the model. It is also important to note that that this model has an R-squared of 0.5875, meaning it explains 58.6% of the dataset. Although this does explain the majority, it is still not very strong.
### Removing non-significant variables
```{r linear2}
multipleLinearRegression2<- lm(Property.Price.... ~ Bedrooms + Bathrooms +
                               Lot.size..m2. + Distance..km.)
summary(multipleLinearRegression2)
```
Here all variables are significant, however our R-squared value is slightly lower at 0.5806. Meaning it explains slightly less of the dataset. This therefore may mean that although these variables are not significant, they do still help define the dataset. 

## Exponential Regression
### Using all variables
```{r exponential1}
exponetialRegression1<- lm(log(Property.Price....) ~ Bedrooms + Bathrooms + 
                                 Car.space + Lot.size..m2. + Distance..km.)
summary(exponetialRegression1)
```
Here all variables are now significant and the R-Squared value is higher at 0.6157, meaning it defines 61.6% of the dataset which is more than the previous model, suggesting it fits the dataset a lot better.


```{r exponential2}
par(mfrow=c(2,2))
plot(exponetialRegression1)
```

Graph 1:
Does not appear to be scattered. Therefore constant variance assumption is not met. Graph 2: (Normal Q-Q)
Appears to be a straight line. Therefore normality assumption is met.
Graph 3:
Same as graph 1.
Graph 4:
There appears to be one influential point, 22, where removing this may improve the model but not yet.
```{r relaimpo }
library(relaimpo)
```

Using the model from previous model building
```{r relaimpo2 }
exponentialRegression <- lm(log(Property.Price....) ~ Bedrooms + Bathrooms + 
                              Car.space + Lot.size..m2. + Distance..km.)
```

```{r relaimpo3 }
calc.relimp(exponentialRegression)
```

Bootstrap of results:
```{r relaimpo4 }
bootresults<-boot.relimp(exponentialRegression, b=1000)
ci<-booteval.relimp(bootresults, norank = T)
plot(ci)
```

## Residuals Vs Distance

```{r resid }
library(ggplot2)
```

rebuild the model but without distance
```{r resid2 }
exponentialRegression_noDistance <- lm(log(Property.Price....) ~ Bedrooms + 
                                         Bathrooms + Car.space + Lot.size..m2.)
```

take residuals
```{r resid3 }
exponentialRegression_noDistance.res = resid(exponentialRegression_noDistance)
```

```{r resid4 }
ggplot(parra, aes(x=Distance..km., y=exponentialRegression_noDistance.res)) +
  geom_point() +
  geom_smooth()
```
To observe there appears to be two change points here. In the first few km (approx 0-3) you could say that there is no change there while also during the last few km (approx 6-9 ) it also begins to flatten out. You can then say that between the two there is a linear relationship between them.

## Change Point Model
Above we observed two possible change points. Below we will only explore the second one (where the lake no longer has an effect). We may pick up on the first change point later.

```{r change }
x <- Distance..km.

y <- log(Property.Price....)
```

```{r change2 }
plot(x,y,xlab="distance",ylab="price")

abline(lm(y~x),cex=2,col="blue")
```
```{r change3 }
ll <- vector()

tau = seq(0,10,0.1)

for (i in 1:length(tau)) {
  tempx <- ifelse(x>tau[i],0,x-tau[i])
  ll[i] <- logLik( lm(y~tempx) )	
}

LR <- 2*(ll - logLik( lm(y~x) ) ) 
tau[which.max(ll)]
```
Change point occurs at 7.4 km

```{r change4 }
plot(tau,LR,type="l",xlab="threshold",ylab="G")

abline(h=3.84)
```

```{r change5 }
max(ll)
tau_est = tau[which.max(ll)]
```

```{r change6 }
plot(tau,LR,type="l",xlab="threshold",ylab="G")

abline(h=3.84)

abline(v=tau_est)
```

```{r change7 }
plot(x,y,xlab="distance",ylab="price")
pred <- predict(lm(y~ifelse(x>tau_est,0,x-tau_est)))
lines(sort(x),pred[order(x)],cex=2,col="blue")
```

Here you can see that there is a linear relationship until we reach the change point at 7.4km.
